{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# make sure you don't hog all the video memory\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "###################################\n",
    "\n",
    "from keras.layers import Input, Lambda, merge, Dense, DepthwiseConv2D, Activation, AveragePooling2D\n",
    "from keras.layers import Flatten,Conv2D, MaxPooling2D, Dropout, BatchNormalization, SeparableConv2D\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (450, 60, 128)\n",
      "Original labels shape: (450, 2)\n"
     ]
    }
   ],
   "source": [
    "file = 'Merged456-197-289_ICA(-eyes)+AUDpreproc.mat, DS2=64Hz, FIR=2-30Hz, centnorm=1, step=2, win=2, TD, 1-93.mat' #0.75(unstable)\n",
    "\n",
    "# get the Dataset:\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "path = '/home/amplifier/home/DATASETS/' + file\n",
    "mat_contents = sio.loadmat(path)\n",
    "X = mat_contents['X']\n",
    "Y = mat_contents['Z']\n",
    "\n",
    "if X.shape[1]<X.shape[2]:\n",
    "    X = np.transpose(X,[0,2,1])\n",
    "\n",
    "if Y.shape[1] > Y.shape[0]:\n",
    "    Y = Y.T\n",
    "\n",
    "    \n",
    "# # one hot encode the labels:\n",
    "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "Y = onehot_encoder.fit_transform(Y)\n",
    "\n",
    "X = X.transpose(0,2,1)\n",
    "\n",
    "print('Original data shape:', X.shape)\n",
    "print('Original labels shape:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 4)\n",
      "(900, 4)\n",
      "xx_train: 720\n",
      "xx_test: 180\n",
      "Transformed to numpy array:\n",
      "xx_train: (720, 3, 60, 128)\n",
      "xx_test: (180, 3, 60, 128)\n"
     ]
    }
   ],
   "source": [
    "a,b = [],[]\n",
    "for i in range(len(Y)):\n",
    "    a.append(i if np.sum((Y[i]-np.array([1,0]))**2)==0 else None)\n",
    "a = [x for x in a if x is not None]\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    b.append(i if np.sum((Y[i]-np.array([0,1]))**2)==0 else None)\n",
    "b = [x for x in b if x is not None]\n",
    "\n",
    "dat = np.zeros([len(X),4])\n",
    "print(dat.shape)\n",
    "dat[:,0] = np.random.choice(a, len(X), replace=True)\n",
    "dat[:,1] = np.random.choice(a, len(X), replace=True)\n",
    "dat[:,2] = np.random.choice(b, len(X), replace=True)\n",
    "dat[:,3] = 0 # a\n",
    "\n",
    "dat1 = np.zeros([len(X),4])\n",
    "dat1[:,0] = np.random.choice(b, len(X), replace=True)\n",
    "dat1[:,1] = np.random.choice(b, len(X), replace=True)\n",
    "dat1[:,2] = np.random.choice(a, len(X), replace=True)\n",
    "dat1[:,3] = 1 # b\n",
    "\n",
    "dat = np.vstack((dat,dat1))\n",
    "print(dat.shape)\n",
    "\n",
    "XX, YY = [], []\n",
    "for i in range(len(dat)):\n",
    "    XX.append([X[dat[i,0].astype(int),:,:], X[dat[i,1].astype(int),:,:], X[dat[i,2].astype(int),:,:]])\n",
    "    YY.append(dat[i,3])\n",
    "\n",
    "XX[1][1].shape\n",
    "xx_train, xx_test, yy_train, yy_test = train_test_split(XX, YY, test_size=0.2, shuffle=True)\n",
    "\n",
    "print('xx_train:', len(xx_train))\n",
    "print('xx_test:', len(xx_test))\n",
    "\n",
    "xx_train = np.asarray(xx_train)\n",
    "xx_test = np.asarray(xx_test)\n",
    "\n",
    "print('Transformed to numpy array:')\n",
    "print('xx_train:', xx_train.shape)\n",
    "print('xx_test:', xx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EEGNet_my(input1, nb_classes, Chans = 64, Samples = 128, \n",
    "             dropoutRate = 0.25, kernLength = 64, F1 = 4, \n",
    "             D = 2, F2 = 8, dropoutType = 'Dropout'):\n",
    "    \n",
    "    \"\"\" Keras Implementation of EEGNet (https://arxiv.org/abs/1611.08024)\n",
    "\n",
    "    Inputs:\n",
    "        \n",
    "      nb_classes      : int, number of classes to classify\n",
    "      Chans, Samples  : number of channels and time points in the EEG data\n",
    "      dropoutRate     : dropout fraction\n",
    "      kernLength      : length of temporal convolution in first layer. We found\n",
    "                        that setting this to be half the sampling rate worked\n",
    "                        well in practice. For the SMR dataset in particular\n",
    "                        since the data was high-passed at 4Hz we used a kernel\n",
    "                        length of 32.     \n",
    "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
    "                        filters (F2) to learn. Default: F1 = 4, F2 = F1 * D. \n",
    "      D               : number of spatial filters to learn within each temporal\n",
    "                        convolution. Default: D = 2\n",
    "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "    \n",
    "    init = RandomUniform(minval=-0.1, maxval=0.1, seed=29)\n",
    "    net = Sequential()\n",
    "    net.add (Conv2D(F1, (1, kernLength), padding = 'same',\n",
    "                                   input_shape = (Chans, Samples,1),\n",
    "                                   use_bias = False, bias_initializer=init, kernel_initializer=init))\n",
    "    net.add (BatchNormalization())\n",
    "    net.add (DepthwiseConv2D((Chans, 1), use_bias = False, \n",
    "                                   depth_multiplier = D,\n",
    "                                   depthwise_constraint = max_norm(1.), bias_initializer=init, kernel_initializer=init))\n",
    "    net.add (BatchNormalization())\n",
    "    net.add (Activation('elu'))\n",
    "    net.add (AveragePooling2D((1, 4)))\n",
    "    net.add (dropoutType(dropoutRate))\n",
    "    net.add (SeparableConv2D(F2, (1, 16), use_bias = False, padding = 'same', bias_initializer=init, kernel_initializer=init))\n",
    "    net.add (BatchNormalization())\n",
    "    net.add (Activation('elu'))\n",
    "    net.add (AveragePooling2D((1, 8)))\n",
    "    net.add (dropoutType(dropoutRate))\n",
    "    net.add (Flatten())\n",
    "    net.add (Dense(nb_classes, kernel_constraint = max_norm(0.25), bias_initializer=init, kernel_initializer=init))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 3, 60, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    norm1 = K.sqrt(K.sum(K.square(y_pred[0] - y_pred[1]), axis=-1, keepdims=True))\n",
    "    norm2 = K.sqrt(K.sum(K.square(y_pred[0] - y_pred[2]), axis=-1, keepdims=True))\n",
    "    loss = norm1 - norm2 + 0.2\n",
    "    return loss\n",
    "\n",
    "# input_shape = (64, 128, 1)\n",
    "input_shape = xx_train[-1,-1,:,:].shape + (1,)\n",
    "\n",
    "a_input = Input(input_shape)\n",
    "r_input = Input(input_shape)\n",
    "n_input = Input(input_shape)\n",
    "\n",
    "#call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "# encoded NOT as Sequential (stack of layers), but as a Tensor!!!! if you add an argument, a Tensor is returned\n",
    "encoded_a = EEGNet_my(input_shape, 10, Chans=input_shape[0])(a_input)\n",
    "encoded_r = EEGNet_my(input_shape, 10, Chans=input_shape[0])(r_input)\n",
    "encoded_n = EEGNet_my(input_shape, 10, Chans=input_shape[0])(n_input)\n",
    "\n",
    "class MergeLegs(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MergeLegs, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self ,x ,mask=None):\n",
    "        a = x[0]\n",
    "        r = x[1]\n",
    "        n = x[2]\n",
    "        norm1 = K.sqrt(K.sum(K.square(a - r), axis=-1, keepdims=True))\n",
    "        norm2 = K.sqrt(K.sum(K.square(a - n), axis=-1, keepdims=True))\n",
    "        loss = norm1 - norm2 + 0.5\n",
    "        self.add_loss(loss, x)\n",
    "        #you can output whatever you need, just update output_shape adequately\n",
    "        #But this is probably useful\n",
    "        return K.concatenate([a,r,n], axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0],1)\n",
    "    \n",
    "# prediction = MergeLegs()([encoded_a, encoded_r, encoded_n])\n",
    "\n",
    "# siamese_net = Model(inputs=[a_input, r_input, n_input], outputs=prediction)\n",
    "# siamese_net.summary()\n",
    "# siamese_net.compile(loss=triplet_loss, optimizer='adam')\n",
    "\n",
    "# # layer to merge two encoded inputs with the l1 distance between them\n",
    "# # L1_layer = Lambda(lambda tensors:K.abs(tensors[0] + tensors[1] + tensors[2]))\n",
    "\n",
    "L1_layer = Lambda(lambda tensors:\n",
    "                  K.sqrt(K.sum(K.square(tensors[0] - tensors[1]), axis=-1, keepdims=True)) -\n",
    "                  K.sqrt(K.sum(K.square(tensors[0] - tensors[2]), axis=-1, keepdims=True)) +\n",
    "                  0.1)\n",
    "\n",
    "# L1_layer = Lambda(lambda tensors:\n",
    "#                   K.tanh(K.sqrt(K.sum(K.square(tensors[0] - tensors[1]), axis=-1, keepdims=True))) -\n",
    "#                   K.tanh(K.sqrt(K.sum(K.square(tensors[0] - tensors[2]), axis=-1, keepdims=True))) +\n",
    "#                   0.1)\n",
    "\n",
    "#call this layer on list of three input tensors.\n",
    "L1_distance = L1_layer([encoded_a, encoded_r, encoded_n])\n",
    "prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "\n",
    "siamese_net = Model(inputs=[a_input, r_input, n_input], outputs=prediction)\n",
    "siamese_net.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.5664 - val_loss: 0.2932\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:435: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.2104 - val_loss: 0.1221\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0722 - val_loss: 0.0684\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0305 - val_loss: 0.0387\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0162 - val_loss: 0.0223\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0109 - val_loss: 0.0138\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0075 - val_loss: 0.0092\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0057 - val_loss: 0.0066\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.0011 - val_loss: 8.9380e-04\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.0011 - val_loss: 7.9384e-04\n",
      "Epoch 20/100\n",
      " - 1s - loss: 9.9048e-04 - val_loss: 7.0620e-04\n",
      "Epoch 21/100\n",
      " - 1s - loss: 9.3271e-04 - val_loss: 6.3689e-04\n",
      "Epoch 22/100\n",
      " - 1s - loss: 9.0316e-04 - val_loss: 5.6968e-04\n",
      "Epoch 23/100\n",
      " - 1s - loss: 7.1354e-04 - val_loss: 5.2265e-04\n",
      "Epoch 24/100\n",
      " - 1s - loss: 6.8687e-04 - val_loss: 4.6849e-04\n",
      "Epoch 25/100\n",
      " - 1s - loss: 7.5446e-04 - val_loss: 4.3637e-04\n",
      "Epoch 26/100\n",
      " - 1s - loss: 6.0441e-04 - val_loss: 4.0587e-04\n",
      "Epoch 27/100\n",
      " - 1s - loss: 5.8546e-04 - val_loss: 3.7708e-04\n",
      "Epoch 28/100\n",
      " - 1s - loss: 5.3577e-04 - val_loss: 3.5341e-04\n",
      "Epoch 29/100\n",
      " - 1s - loss: 4.8797e-04 - val_loss: 3.3169e-04\n",
      "Epoch 30/100\n",
      " - 1s - loss: 4.8949e-04 - val_loss: 3.0732e-04\n",
      "Epoch 31/100\n",
      " - 1s - loss: 4.2530e-04 - val_loss: 2.9082e-04\n",
      "Epoch 32/100\n",
      " - 1s - loss: 4.1947e-04 - val_loss: 2.7256e-04\n",
      "Epoch 33/100\n",
      " - 1s - loss: 4.5344e-04 - val_loss: 2.5588e-04\n",
      "Epoch 34/100\n",
      " - 1s - loss: 4.2531e-04 - val_loss: 2.3489e-04\n",
      "Epoch 35/100\n",
      " - 1s - loss: 4.0001e-04 - val_loss: 2.2173e-04\n",
      "Epoch 36/100\n",
      " - 1s - loss: 3.4748e-04 - val_loss: 2.0925e-04\n",
      "Epoch 37/100\n",
      " - 1s - loss: 3.1976e-04 - val_loss: 1.9553e-04\n",
      "Epoch 38/100\n",
      " - 1s - loss: 3.2806e-04 - val_loss: 1.8707e-04\n",
      "Epoch 39/100\n",
      " - 1s - loss: 3.3222e-04 - val_loss: 1.7393e-04\n",
      "Epoch 40/100\n",
      " - 1s - loss: 2.8296e-04 - val_loss: 1.6866e-04\n",
      "Epoch 41/100\n",
      " - 1s - loss: 2.7345e-04 - val_loss: 1.5956e-04\n",
      "Epoch 42/100\n",
      " - 1s - loss: 2.7926e-04 - val_loss: 1.4940e-04\n",
      "Epoch 43/100\n",
      " - 1s - loss: 2.5967e-04 - val_loss: 1.4245e-04\n",
      "Epoch 44/100\n",
      " - 1s - loss: 2.6857e-04 - val_loss: 1.3786e-04\n",
      "Epoch 45/100\n",
      " - 1s - loss: 2.2188e-04 - val_loss: 1.2993e-04\n",
      "Epoch 46/100\n",
      " - 1s - loss: 2.1899e-04 - val_loss: 1.2407e-04\n",
      "Epoch 47/100\n",
      " - 1s - loss: 2.2058e-04 - val_loss: 1.1748e-04\n",
      "Epoch 48/100\n",
      " - 1s - loss: 2.1766e-04 - val_loss: 1.1351e-04\n",
      "Epoch 49/100\n",
      " - 1s - loss: 1.8929e-04 - val_loss: 1.0618e-04\n",
      "Epoch 50/100\n",
      " - 1s - loss: 2.1703e-04 - val_loss: 1.0146e-04\n",
      "Epoch 51/100\n",
      " - 1s - loss: 1.9297e-04 - val_loss: 9.8084e-05\n",
      "Epoch 52/100\n",
      " - 1s - loss: 2.1167e-04 - val_loss: 9.4833e-05\n",
      "Epoch 53/100\n",
      " - 1s - loss: 1.6103e-04 - val_loss: 9.3511e-05\n",
      "Epoch 54/100\n",
      " - 1s - loss: 1.7644e-04 - val_loss: 9.0232e-05\n",
      "Epoch 55/100\n",
      " - 1s - loss: 1.8834e-04 - val_loss: 8.5367e-05\n",
      "Epoch 56/100\n",
      " - 1s - loss: 1.6714e-04 - val_loss: 8.0206e-05\n",
      "Epoch 57/100\n",
      " - 1s - loss: 1.3662e-04 - val_loss: 7.7077e-05\n",
      "Epoch 58/100\n",
      " - 1s - loss: 1.5301e-04 - val_loss: 7.4858e-05\n",
      "Epoch 59/100\n",
      " - 1s - loss: 1.4088e-04 - val_loss: 7.1572e-05\n",
      "Epoch 60/100\n",
      " - 1s - loss: 1.3685e-04 - val_loss: 6.8045e-05\n",
      "Epoch 61/100\n",
      " - 1s - loss: 1.2069e-04 - val_loss: 6.5718e-05\n",
      "Epoch 62/100\n",
      " - 1s - loss: 1.0826e-04 - val_loss: 6.1918e-05\n",
      "Epoch 63/100\n",
      " - 1s - loss: 1.2114e-04 - val_loss: 6.0089e-05\n",
      "Epoch 64/100\n",
      " - 1s - loss: 1.2662e-04 - val_loss: 5.8802e-05\n",
      "Epoch 65/100\n",
      " - 1s - loss: 1.3411e-04 - val_loss: 5.6612e-05\n",
      "Epoch 66/100\n",
      " - 1s - loss: 1.2225e-04 - val_loss: 5.3198e-05\n",
      "Epoch 67/100\n",
      " - 1s - loss: 1.2013e-04 - val_loss: 5.0511e-05\n",
      "Epoch 68/100\n",
      " - 1s - loss: 1.1068e-04 - val_loss: 4.9063e-05\n",
      "Epoch 69/100\n",
      " - 1s - loss: 8.9352e-05 - val_loss: 4.7522e-05\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f01bce2286b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mock_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 callbacks=[checkpointer, early_stopping])\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# model.save('/home/amplifier/home/NEW_DL/models/EEGnet.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Siamese model:\n",
    "train_mock_labels = np.zeros(len(xx_train))\n",
    "test_mock_labels = np.zeros(len(xx_test))\n",
    "\n",
    "# Training time!\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100, mode='min')\n",
    "checkpointer = ModelCheckpoint(filepath='/home/amplifier/home/NEW_DL/weights/Siamese.h5',\n",
    "                               verbose=1,\n",
    "                               monitor='val_acc',\n",
    "                               save_best_only=True)\n",
    "\n",
    "train_history = siamese_net.fit(x = [xx_train[:,0,:,:,None], xx_train[:,1,:,:,None], xx_train[:,2,:,:,None]],\n",
    "                y = train_mock_labels,\n",
    "                epochs=100,\n",
    "                batch_size=20,\n",
    "                verbose=2,\n",
    "                shuffle=True,\n",
    "                validation_data=([xx_test[:,0,:,:,None], xx_test[:,1,:,:,None], xx_test[:,2,:,:,None]], test_mock_labels),\n",
    "                callbacks=[checkpointer, early_stopping])\n",
    "\n",
    "# model.save('/home/amplifier/home/NEW_DL/models/EEGnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 60, 128, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 60, 128, 4)        256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 60, 128, 4)        16        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_11 (Depthwi (None, 1, 128, 8)         480       \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 1, 128, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1, 128, 8)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_21 (Averag (None, 1, 32, 8)          0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1, 32, 8)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_11 (Separab (None, 1, 32, 8)          192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 1, 32, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 1, 32, 8)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_22 (Averag (None, 1, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 1, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,338\n",
      "Trainable params: 1,298\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n",
      "['sequential_21', 'sequential_22', 'sequential_23']\n",
      "['dense_14', 'dense_15', 'dense_16']\n",
      "0.790569 0.790569 0.790569\n",
      "\n",
      "In the models made out the layers take from the siamese model:\n",
      "dense_14 dense_15 dense_16\n",
      "0.790569 0.790569 0.790569\n"
     ]
    }
   ],
   "source": [
    "testmod_1 = Sequential()\n",
    "testmod_1.add(siamese_net.layers[0])\n",
    "for i in range(14):\n",
    "    testmod_1.add(siamese_net.layers[3].layers[i])\n",
    "testmod_1.summary()\n",
    "\n",
    "testmod_2 = Sequential()\n",
    "testmod_2.add(siamese_net.layers[0])\n",
    "for i in range(14):\n",
    "    testmod_2.add(siamese_net.layers[4].layers[i])\n",
    "    \n",
    "testmod_3 = Sequential()\n",
    "testmod_3.add(siamese_net.layers[0])\n",
    "for i in range(14):\n",
    "    testmod_3.add(siamese_net.layers[5].layers[i])\n",
    "\n",
    "pred_1 = testmod_1.predict(xx_train[:,0,:,:,None])\n",
    "pred_2 = testmod_2.predict(xx_train[:,0,:,:,None])\n",
    "pred_3 = testmod_3.predict(xx_train[:,0,:,:,None])\n",
    "\n",
    "# check if the weights in the siamese are really the same in all the three leg of the model:\n",
    "a1 = np.linalg.norm(siamese_net.layers[3].layers[13].get_weights()[0])\n",
    "a2 = np.linalg.norm(siamese_net.layers[4].layers[13].get_weights()[0])\n",
    "a3 = np.linalg.norm(siamese_net.layers[5].layers[13].get_weights()[0])\n",
    "print([siamese_net.layers[x].name for x in range(3,6)])\n",
    "print([siamese_net.layers[x].layers[13].name for x in range(3,6)])\n",
    "print(a1,a2,a3)\n",
    "print('\\nIn the models made out the layers take from the siamese model:')\n",
    "a1 = np.linalg.norm(testmod_1.layers[14].get_weights()[0])\n",
    "a2 = np.linalg.norm(testmod_2.layers[14].get_weights()[0])\n",
    "a3 = np.linalg.norm(testmod_3.layers[14].get_weights()[0])\n",
    "print(testmod_1.layers[14].name,testmod_2.layers[14].name,testmod_3.layers[14].name)\n",
    "print(a1,a2,a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60, 128, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train[None,0,0,:,:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "result: \t 0 \t 1.0 \t 0.0\n",
      "result: \t 1 \t 0.0 \t 0.0515698\n",
      "result: \t 2 \t 1.0 \t 0.0625995\n",
      "result: \t 3 \t 0.0 \t 0.387477\n",
      "result: \t 4 \t 1.0 \t 0.274205\n",
      "result: \t 5 \t 1.0 \t 0.299221\n",
      "result: \t 6 \t 1.0 \t 0.566591\n",
      "result: \t 7 \t 0.0 \t 0.237894\n",
      "result: \t 8 \t 1.0 \t 0.515248\n",
      "result: \t 9 \t 1.0 \t 0.48981\n",
      "result: \t 10 \t 0.0 \t 0.776937\n",
      "result: \t 11 \t 0.0 \t 0.127648\n",
      "result: \t 12 \t 1.0 \t 0.31852\n",
      "result: \t 13 \t 0.0 \t 0.0748979\n",
      "result: \t 14 \t 0.0 \t 0.156602\n",
      "result: \t 15 \t 0.0 \t 0.600319\n",
      "result: \t 16 \t 0.0 \t 0.686846\n",
      "result: \t 17 \t 1.0 \t 0.0781404\n",
      "result: \t 18 \t 0.0 \t 0.196362\n",
      "result: \t 19 \t 0.0 \t 0.272538\n",
      "result: \t 20 \t 0.0 \t 0.304549\n",
      "result: \t 21 \t 0.0 \t 0.36488\n",
      "result: \t 22 \t 1.0 \t 0.48981\n",
      "result: \t 23 \t 1.0 \t 0.462892\n",
      "result: \t 24 \t 0.0 \t 0.238791\n",
      "result: \t 25 \t 1.0 \t 0.0233032\n",
      "result: \t 26 \t 0.0 \t 0.279878\n",
      "result: \t 27 \t 0.0 \t 0.721961\n",
      "result: \t 28 \t 0.0 \t 0.0767553\n",
      "result: \t 29 \t 0.0 \t 0.450621\n",
      "result: \t 30 \t 0.0 \t 0.478036\n",
      "result: \t 31 \t 0.0 \t 0.161759\n",
      "result: \t 32 \t 0.0 \t 0.243627\n",
      "result: \t 33 \t 1.0 \t 0.669203\n",
      "result: \t 34 \t 1.0 \t 0.157507\n",
      "result: \t 35 \t 0.0 \t 0.0514403\n",
      "result: \t 36 \t 1.0 \t 0.776013\n",
      "result: \t 37 \t 1.0 \t 0.190493\n",
      "result: \t 38 \t 1.0 \t 0.384203\n",
      "result: \t 39 \t 1.0 \t 0.0386558\n",
      "result: \t 40 \t 0.0 \t 0.128527\n",
      "result: \t 41 \t 1.0 \t 0.220307\n",
      "result: \t 42 \t 1.0 \t 0.175173\n",
      "result: \t 43 \t 0.0 \t 0.527769\n",
      "result: \t 44 \t 0.0 \t 0.128527\n",
      "result: \t 45 \t 0.0 \t 0.2898\n",
      "result: \t 46 \t 0.0 \t 0.38612\n",
      "result: \t 47 \t 0.0 \t 0.439551\n",
      "result: \t 48 \t 0.0 \t 0.124393\n",
      "result: \t 49 \t 1.0 \t 0.141949\n",
      "result: \t 50 \t 0.0 \t 0.350796\n",
      "result: \t 51 \t 1.0 \t 0.372468\n",
      "result: \t 52 \t 0.0 \t 0.390913\n",
      "result: \t 53 \t 1.0 \t 0.0375096\n",
      "result: \t 54 \t 0.0 \t 0.108073\n",
      "result: \t 55 \t 0.0 \t 0.388102\n",
      "result: \t 56 \t 0.0 \t 0.0316148\n",
      "result: \t 57 \t 1.0 \t 0.281989\n",
      "result: \t 58 \t 0.0 \t 0.289819\n",
      "result: \t 59 \t 0.0 \t 0.319385\n",
      "result: \t 60 \t 1.0 \t 0.238855\n",
      "result: \t 61 \t 0.0 \t 0.107033\n",
      "result: \t 62 \t 0.0 \t 0.406361\n",
      "result: \t 63 \t 1.0 \t 0.0772082\n",
      "result: \t 64 \t 0.0 \t 0.326625\n",
      "result: \t 65 \t 1.0 \t 0.0925119\n",
      "result: \t 66 \t 0.0 \t 0.00882445\n",
      "result: \t 67 \t 1.0 \t 0.515248\n",
      "result: \t 68 \t 1.0 \t 0.193332\n",
      "result: \t 69 \t 0.0 \t 0.334237\n",
      "result: \t 70 \t 0.0 \t 0.172045\n",
      "result: \t 71 \t 0.0 \t 0.0718109\n",
      "result: \t 72 \t 1.0 \t 0.516536\n",
      "result: \t 73 \t 0.0 \t 0.0857466\n",
      "result: \t 74 \t 1.0 \t 0.190493\n",
      "result: \t 75 \t 0.0 \t 0.405695\n",
      "result: \t 76 \t 1.0 \t 0.516536\n",
      "result: \t 77 \t 0.0 \t 0.387477\n",
      "result: \t 78 \t 0.0 \t 0.383709\n",
      "result: \t 79 \t 0.0 \t 0.0638163\n",
      "result: \t 80 \t 1.0 \t 0.299221\n",
      "result: \t 81 \t 0.0 \t 0.67458\n",
      "result: \t 82 \t 0.0 \t 0.272753\n",
      "result: \t 83 \t 1.0 \t 0.332691\n",
      "result: \t 84 \t 0.0 \t 0.150298\n",
      "result: \t 85 \t 0.0 \t 0.0353435\n",
      "result: \t 86 \t 0.0 \t 0.238791\n",
      "result: \t 87 \t 1.0 \t 0.308873\n",
      "result: \t 88 \t 0.0 \t 0.170627\n",
      "result: \t 89 \t 1.0 \t 0.0767959\n",
      "result: \t 90 \t 0.0 \t 0.248941\n",
      "result: \t 91 \t 0.0 \t 0.0887147\n",
      "result: \t 92 \t 1.0 \t 0.0201837\n",
      "result: \t 93 \t 1.0 \t 0.120068\n",
      "result: \t 94 \t 1.0 \t 0.643144\n",
      "result: \t 95 \t 0.0 \t 0.38323\n",
      "result: \t 96 \t 0.0 \t 0.537271\n",
      "result: \t 97 \t 1.0 \t 0.228627\n",
      "result: \t 98 \t 1.0 \t 0.462892\n",
      "result: \t 99 \t 0.0 \t 0.0859175\n",
      "result: \t 100 \t 0.0 \t 0.545302\n",
      "result: \t 101 \t 1.0 \t 0.589467\n",
      "result: \t 102 \t 0.0 \t 0.246166\n",
      "result: \t 103 \t 1.0 \t 0.900846\n",
      "result: \t 104 \t 1.0 \t 0.659832\n",
      "result: \t 105 \t 0.0 \t 0.195603\n",
      "result: \t 106 \t 1.0 \t 0.541203\n",
      "result: \t 107 \t 0.0 \t 0.0748979\n",
      "result: \t 108 \t 1.0 \t 0.627372\n",
      "result: \t 109 \t 1.0 \t 0.162906\n",
      "result: \t 110 \t 1.0 \t 0.0490841\n",
      "result: \t 111 \t 1.0 \t 0.126146\n",
      "result: \t 112 \t 1.0 \t 0.452516\n",
      "result: \t 113 \t 1.0 \t 0.914794\n",
      "result: \t 114 \t 0.0 \t 0.541516\n",
      "result: \t 115 \t 1.0 \t 0.501117\n",
      "result: \t 116 \t 1.0 \t 0.0256016\n",
      "result: \t 117 \t 0.0 \t 0.15043\n",
      "result: \t 118 \t 1.0 \t 0.515248\n",
      "result: \t 119 \t 1.0 \t 0.301462\n",
      "result: \t 120 \t 0.0 \t 0.485448\n",
      "result: \t 121 \t 0.0 \t 0.244196\n",
      "result: \t 122 \t 0.0 \t 0.350796\n",
      "result: \t 123 \t 1.0 \t 0.22235\n",
      "result: \t 124 \t 0.0 \t 0.271647\n",
      "result: \t 125 \t 1.0 \t 0.669203\n",
      "result: \t 126 \t 1.0 \t 0.610738\n",
      "result: \t 127 \t 0.0 \t 0.103355\n",
      "result: \t 128 \t 1.0 \t 0.643144\n",
      "result: \t 129 \t 0.0 \t 0.237894\n",
      "result: \t 130 \t 0.0 \t 0.411157\n",
      "result: \t 131 \t 1.0 \t 0.281989\n",
      "result: \t 132 \t 1.0 \t 0.627372\n",
      "result: \t 133 \t 0.0 \t 0.171171\n",
      "result: \t 134 \t 0.0 \t 0.450621\n",
      "result: \t 135 \t 0.0 \t 0.315816\n",
      "result: \t 136 \t 1.0 \t 0.162906\n",
      "result: \t 137 \t 1.0 \t 0.372274\n",
      "result: \t 138 \t 0.0 \t 0.326625\n",
      "result: \t 139 \t 1.0 \t 0.720241\n",
      "result: \t 140 \t 1.0 \t 0.274205\n",
      "result: \t 141 \t 0.0 \t 0.170735\n",
      "result: \t 142 \t 0.0 \t 0.0514403\n",
      "result: \t 143 \t 0.0 \t 0.352948\n",
      "result: \t 144 \t 0.0 \t 0.661681\n",
      "result: \t 145 \t 0.0 \t 0.173435\n",
      "result: \t 146 \t 0.0 \t 0.165139\n",
      "result: \t 147 \t 1.0 \t 0.175058\n",
      "result: \t 148 \t 0.0 \t 0.243627\n",
      "result: \t 149 \t 1.0 \t 0.241885\n",
      "result: \t 150 \t 1.0 \t 0.193642\n",
      "result: \t 151 \t 1.0 \t 0.0840588\n",
      "result: \t 152 \t 1.0 \t 0.013195\n",
      "result: \t 153 \t 0.0 \t 0.248867\n",
      "result: \t 154 \t 1.0 \t 0.281989\n",
      "result: \t 155 \t 0.0 \t 0.0233602\n",
      "result: \t 156 \t 1.0 \t 0.0772082\n",
      "result: \t 157 \t 1.0 \t 0.659832\n",
      "result: \t 158 \t 0.0 \t 0.0439283\n",
      "result: \t 159 \t 1.0 \t 0.126146\n",
      "result: \t 160 \t 0.0 \t 0.159662\n",
      "result: \t 161 \t 0.0 \t 0.545302\n",
      "result: \t 162 \t 1.0 \t 0.48981\n",
      "result: \t 163 \t 0.0 \t 0.260196\n",
      "result: \t 164 \t 0.0 \t 0.418144\n",
      "result: \t 165 \t 0.0 \t 0.280865\n",
      "result: \t 166 \t 1.0 \t 0.169597\n",
      "result: \t 167 \t 1.0 \t 1.17717\n",
      "result: \t 168 \t 0.0 \t 0.504748\n",
      "result: \t 169 \t 0.0 \t 0.171171\n",
      "result: \t 170 \t 0.0 \t 0.0178539\n",
      "result: \t 171 \t 0.0 \t 0.723239\n",
      "result: \t 172 \t 1.0 \t 0.0856002\n",
      "result: \t 173 \t 0.0 \t 0.304549\n",
      "result: \t 174 \t 1.0 \t 0.0233032\n",
      "result: \t 175 \t 1.0 \t 0.132931\n",
      "result: \t 176 \t 1.0 \t 0.217074\n",
      "result: \t 177 \t 1.0 \t 0.217599\n",
      "result: \t 178 \t 1.0 \t 0.299221\n",
      "result: \t 179 \t 0.0 \t 0.289819\n",
      "result: \t 180 \t 1.0 \t 0.238855\n",
      "result: \t 181 \t 0.0 \t 0.301519\n",
      "result: \t 182 \t 1.0 \t 0.175173\n",
      "result: \t 183 \t 1.0 \t 0.00257158\n",
      "result: \t 184 \t 0.0 \t 0.260196\n",
      "result: \t 185 \t 1.0 \t 1.17717\n",
      "result: \t 186 \t 0.0 \t 0.600319\n",
      "result: \t 187 \t 1.0 \t 0.295858\n",
      "result: \t 188 \t 0.0 \t 0.420134\n",
      "result: \t 189 \t 1.0 \t 0.241885\n",
      "result: \t 190 \t 1.0 \t 0.318688\n",
      "result: \t 191 \t 1.0 \t 0.234928\n",
      "result: \t 192 \t 0.0 \t 0.140024\n",
      "result: \t 193 \t 1.0 \t 0.153328\n",
      "result: \t 194 \t 0.0 \t 0.146735\n",
      "result: \t 195 \t 1.0 \t 0.103986\n",
      "result: \t 196 \t 0.0 \t 0.410932\n",
      "result: \t 197 \t 1.0 \t 0.316605\n",
      "result: \t 198 \t 0.0 \t 0.165139\n",
      "result: \t 199 \t 0.0 \t 0.135082\n",
      "result: \t 200 \t 1.0 \t 0.00384252\n",
      "result: \t 201 \t 1.0 \t 0.232513\n",
      "result: \t 202 \t 0.0 \t 0.470103\n",
      "result: \t 203 \t 1.0 \t 0.0767959\n",
      "result: \t 204 \t 1.0 \t 0.368876\n",
      "result: \t 205 \t 0.0 \t 0.67458\n",
      "result: \t 206 \t 1.0 \t 0.0219877\n",
      "result: \t 207 \t 0.0 \t 0.36488\n",
      "result: \t 208 \t 1.0 \t 0.0490841\n",
      "result: \t 209 \t 0.0 \t 0.284708\n",
      "result: \t 210 \t 1.0 \t 0.610738\n",
      "result: \t 211 \t 1.0 \t 0.589627\n",
      "result: \t 212 \t 0.0 \t 0.284476\n",
      "result: \t 213 \t 1.0 \t 0.368876\n",
      "result: \t 214 \t 1.0 \t 0.096921\n",
      "result: \t 215 \t 0.0 \t 0.00882445\n",
      "result: \t 216 \t 1.0 \t 0.105786\n",
      "result: \t 217 \t 0.0 \t 0.212809\n",
      "result: \t 218 \t 0.0 \t 0.103355\n",
      "result: \t 219 \t 0.0 \t 0.486333\n",
      "result: \t 220 \t 1.0 \t 0.173787\n",
      "result: \t 221 \t 1.0 \t 0.31852\n",
      "result: \t 222 \t 0.0 \t 0.558284\n",
      "result: \t 223 \t 0.0 \t 0.400207\n",
      "result: \t 224 \t 0.0 \t 0.237607\n",
      "result: \t 225 \t 0.0 \t 0.430273\n",
      "result: \t 226 \t 0.0 \t 0.563043\n",
      "result: \t 227 \t 0.0 \t 0.127648\n",
      "result: \t 228 \t 0.0 \t 0.485448\n",
      "result: \t 229 \t 0.0 \t 0.557178\n",
      "result: \t 230 \t 0.0 \t 0.62881\n",
      "result: \t 231 \t 0.0 \t 0.0626878\n",
      "result: \t 232 \t 0.0 \t 0.0526275\n",
      "result: \t 233 \t 1.0 \t 0.433377\n",
      "result: \t 234 \t 1.0 \t 0.523221\n",
      "result: \t 235 \t 1.0 \t 0.191435\n",
      "result: \t 236 \t 1.0 \t 0.126146\n",
      "result: \t 237 \t 0.0 \t 0.00882445\n",
      "result: \t 238 \t 0.0 \t 0.103355\n",
      "result: \t 239 \t 0.0 \t 0.254699\n",
      "result: \t 240 \t 1.0 \t 0.129336\n",
      "result: \t 241 \t 1.0 \t 0.401551\n",
      "result: \t 242 \t 1.0 \t 0.368876\n",
      "result: \t 243 \t 1.0 \t 0.503358\n",
      "result: \t 244 \t 0.0 \t 0.383709\n",
      "result: \t 245 \t 1.0 \t 0.115807\n",
      "result: \t 246 \t 1.0 \t 0.308873\n",
      "result: \t 247 \t 1.0 \t 0.796424\n",
      "result: \t 248 \t 1.0 \t 0.282201\n",
      "result: \t 249 \t 0.0 \t 0.0353435\n",
      "result: \t 250 \t 0.0 \t 0.0422513\n",
      "result: \t 251 \t 1.0 \t 0.338181\n",
      "result: \t 252 \t 1.0 \t 0.462892\n",
      "result: \t 253 \t 1.0 \t 0.193332\n",
      "result: \t 254 \t 0.0 \t 0.597644\n",
      "result: \t 255 \t 0.0 \t 0.404642\n",
      "result: \t 256 \t 0.0 \t 0.420134\n",
      "result: \t 257 \t 0.0 \t 0.0102732\n",
      "result: \t 258 \t 0.0 \t 0.135018\n",
      "result: \t 259 \t 0.0 \t 0.527769\n",
      "result: \t 260 \t 1.0 \t 0.451347\n",
      "result: \t 261 \t 1.0 \t 0.401551\n",
      "result: \t 262 \t 0.0 \t 0.140024\n",
      "result: \t 263 \t 0.0 \t 0.334237\n",
      "result: \t 264 \t 0.0 \t 0.239574\n",
      "result: \t 265 \t 0.0 \t 0.439551\n",
      "result: \t 266 \t 1.0 \t 0.335456\n",
      "result: \t 267 \t 1.0 \t 0.193332\n",
      "result: \t 268 \t 1.0 \t 0.260242\n",
      "result: \t 269 \t 0.0 \t 0.501636\n",
      "result: \t 270 \t 1.0 \t 0.175058\n",
      "result: \t 271 \t 1.0 \t 0.627415\n",
      "result: \t 272 \t 1.0 \t 0.446635\n",
      "result: \t 273 \t 0.0 \t 0.212809\n",
      "result: \t 274 \t 0.0 \t 0.272728\n",
      "result: \t 275 \t 1.0 \t 0.00257158\n",
      "result: \t 276 \t 1.0 \t 0.566591\n",
      "result: \t 277 \t 1.0 \t 0.317931\n",
      "result: \t 278 \t 0.0 \t 0.485448\n",
      "result: \t 279 \t 0.0 \t 0.0785337\n",
      "result: \t 280 \t 0.0 \t 0.395106\n",
      "result: \t 281 \t 0.0 \t 0.272538\n",
      "result: \t 282 \t 1.0 \t 0.105786\n",
      "result: \t 283 \t 0.0 \t 0.279878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: \t 284 \t 1.0 \t 0.319343\n",
      "result: \t 285 \t 1.0 \t 0.175058\n",
      "result: \t 286 \t 1.0 \t 0.566591\n",
      "result: \t 287 \t 1.0 \t 0.0\n",
      "result: \t 288 \t 0.0 \t 0.11456\n",
      "result: \t 289 \t 0.0 \t 0.254699\n",
      "result: \t 290 \t 1.0 \t 0.0219877\n",
      "result: \t 291 \t 0.0 \t 0.124393\n",
      "result: \t 292 \t 0.0 \t 0.288481\n",
      "result: \t 293 \t 0.0 \t 0.171171\n",
      "result: \t 294 \t 1.0 \t 0.217958\n",
      "result: \t 295 \t 1.0 \t 0.229719\n",
      "result: \t 296 \t 0.0 \t 0.218822\n",
      "result: \t 297 \t 0.0 \t 0.0824586\n",
      "result: \t 298 \t 0.0 \t 0.289844\n",
      "result: \t 299 \t 1.0 \t 0.316631\n",
      "result: \t 300 \t 0.0 \t 0.108073\n",
      "result: \t 301 \t 1.0 \t 0.0256016\n",
      "result: \t 302 \t 1.0 \t 0.516536\n",
      "result: \t 303 \t 0.0 \t 0.236628\n",
      "result: \t 304 \t 0.0 \t 0.105843\n",
      "result: \t 305 \t 0.0 \t 0.0353435\n",
      "result: \t 306 \t 0.0 \t 0.260196\n",
      "result: \t 307 \t 0.0 \t 0.289844\n",
      "result: \t 308 \t 0.0 \t 0.0939713\n",
      "result: \t 309 \t 0.0 \t 0.272538\n",
      "result: \t 310 \t 0.0 \t 0.0102732\n",
      "result: \t 311 \t 0.0 \t 0.36488\n",
      "result: \t 312 \t 0.0 \t 0.0526275\n",
      "result: \t 313 \t 0.0 \t 0.420134\n",
      "result: \t 314 \t 0.0 \t 0.269203\n",
      "result: \t 315 \t 1.0 \t 0.096921\n",
      "result: \t 316 \t 0.0 \t 0.541516\n",
      "result: \t 317 \t 1.0 \t 0.265643\n",
      "result: \t 318 \t 1.0 \t 0.0243247\n",
      "result: \t 319 \t 1.0 \t 0.0\n",
      "result: \t 320 \t 0.0 \t 0.161759\n",
      "result: \t 321 \t 0.0 \t 0.334237\n",
      "result: \t 322 \t 0.0 \t 0.541516\n",
      "result: \t 323 \t 0.0 \t 0.0785337\n",
      "result: \t 324 \t 1.0 \t 0.24641\n",
      "result: \t 325 \t 1.0 \t 0.162906\n",
      "result: \t 326 \t 0.0 \t 0.450621\n",
      "result: \t 327 \t 0.0 \t 0.194357\n",
      "result: \t 328 \t 0.0 \t 0.352948\n",
      "result: \t 329 \t 1.0 \t 0.153328\n",
      "result: \t 330 \t 0.0 \t 0.195603\n",
      "result: \t 331 \t 1.0 \t 0.900846\n",
      "result: \t 332 \t 1.0 \t 0.452516\n",
      "result: \t 333 \t 1.0 \t 0.456805\n",
      "result: \t 334 \t 0.0 \t 0.807196\n",
      "result: \t 335 \t 1.0 \t 0.413021\n",
      "result: \t 336 \t 0.0 \t 0.237655\n",
      "result: \t 337 \t 1.0 \t 0.677412\n",
      "result: \t 338 \t 1.0 \t 0.566591\n",
      "result: \t 339 \t 0.0 \t 0.345762\n",
      "result: \t 340 \t 0.0 \t 0.485448\n",
      "result: \t 341 \t 1.0 \t 0.0201837\n",
      "result: \t 342 \t 0.0 \t 0.135986\n",
      "result: \t 343 \t 1.0 \t 0.105487\n",
      "result: \t 344 \t 1.0 \t 0.0479226\n",
      "result: \t 345 \t 0.0 \t 0.246166\n",
      "result: \t 346 \t 1.0 \t 0.0555284\n",
      "result: \t 347 \t 0.0 \t 0.143676\n",
      "result: \t 348 \t 0.0 \t 0.135082\n",
      "result: \t 349 \t 1.0 \t 0.546621\n",
      "result: \t 350 \t 0.0 \t 0.218013\n",
      "result: \t 351 \t 0.0 \t 0.173189\n",
      "result: \t 352 \t 0.0 \t 0.107297\n",
      "result: \t 353 \t 0.0 \t 0.271647\n",
      "result: \t 354 \t 1.0 \t 0.372468\n",
      "result: \t 355 \t 0.0 \t 0.0233248\n",
      "result: \t 356 \t 1.0 \t 0.40439\n",
      "result: \t 357 \t 0.0 \t 0.404642\n",
      "result: \t 358 \t 1.0 \t 0.0840588\n",
      "result: \t 359 \t 1.0 \t 0.170875\n",
      "result: \t 360 \t 0.0 \t 0.251173\n",
      "result: \t 361 \t 1.0 \t 0.971814\n",
      "result: \t 362 \t 1.0 \t 0.19668\n",
      "result: \t 363 \t 1.0 \t 0.0137177\n",
      "result: \t 364 \t 1.0 \t 0.299221\n",
      "result: \t 365 \t 0.0 \t 0.196362\n",
      "result: \t 366 \t 1.0 \t 0.627415\n",
      "result: \t 367 \t 1.0 \t 0.0843563\n",
      "result: \t 368 \t 0.0 \t 0.423798\n",
      "result: \t 369 \t 1.0 \t 0.201754\n",
      "result: \t 370 \t 0.0 \t 0.383709\n",
      "result: \t 371 \t 1.0 \t 0.0103172\n",
      "result: \t 372 \t 1.0 \t 0.589467\n",
      "result: \t 373 \t 1.0 \t 0.477827\n",
      "result: \t 374 \t 1.0 \t 0.269956\n",
      "result: \t 375 \t 1.0 \t 0.269956\n",
      "result: \t 376 \t 1.0 \t 0.11483\n",
      "result: \t 377 \t 0.0 \t 0.576419\n",
      "result: \t 378 \t 1.0 \t 0.0691199\n",
      "result: \t 379 \t 0.0 \t 0.345152\n",
      "result: \t 380 \t 0.0 \t 0.0353435\n",
      "result: \t 381 \t 1.0 \t 0.261924\n",
      "result: \t 382 \t 1.0 \t 0.368876\n",
      "result: \t 383 \t 1.0 \t 0.238855\n",
      "result: \t 384 \t 0.0 \t 0.218013\n",
      "result: \t 385 \t 1.0 \t 0.0721825\n",
      "result: \t 386 \t 1.0 \t 0.318688\n",
      "result: \t 387 \t 1.0 \t 0.335456\n",
      "result: \t 388 \t 1.0 \t 0.231198\n",
      "result: \t 389 \t 0.0 \t 0.212809\n",
      "result: \t 390 \t 0.0 \t 0.0112446\n",
      "result: \t 391 \t 1.0 \t 0.00257158\n",
      "result: \t 392 \t 1.0 \t 0.473502\n",
      "result: \t 393 \t 1.0 \t 0.190493\n",
      "result: \t 394 \t 1.0 \t 0.15579\n",
      "result: \t 395 \t 1.0 \t 0.0386558\n",
      "result: \t 396 \t 0.0 \t 0.723239\n",
      "result: \t 397 \t 0.0 \t 0.161759\n",
      "result: \t 398 \t 0.0 \t 0.289844\n",
      "result: \t 399 \t 0.0 \t 0.0859175\n",
      "result: \t 400 \t 1.0 \t 0.720241\n",
      "result: \t 401 \t 1.0 \t 0.103986\n",
      "result: \t 402 \t 1.0 \t 0.00257158\n",
      "result: \t 403 \t 1.0 \t 0.0206666\n",
      "result: \t 404 \t 1.0 \t 0.0206666\n",
      "result: \t 405 \t 0.0 \t 0.103554\n",
      "result: \t 406 \t 0.0 \t 0.486333\n",
      "result: \t 407 \t 0.0 \t 0.15043\n",
      "result: \t 408 \t 1.0 \t 0.0584177\n",
      "result: \t 409 \t 0.0 \t 0.387477\n",
      "result: \t 410 \t 0.0 \t 0.315816\n",
      "result: \t 411 \t 1.0 \t 0.421897\n",
      "result: \t 412 \t 1.0 \t 0.289414\n",
      "result: \t 413 \t 1.0 \t 0.308873\n",
      "result: \t 414 \t 0.0 \t 0.107033\n",
      "result: \t 415 \t 1.0 \t 0.105786\n",
      "result: \t 416 \t 0.0 \t 0.537271\n",
      "result: \t 417 \t 1.0 \t 0.270856\n",
      "result: \t 418 \t 0.0 \t 0.105843\n",
      "result: \t 419 \t 1.0 \t 0.0176077\n",
      "result: \t 420 \t 0.0 \t 0.0526275\n",
      "result: \t 421 \t 0.0 \t 0.0857466\n",
      "result: \t 422 \t 1.0 \t 0.316631\n",
      "result: \t 423 \t 0.0 \t 0.272605\n",
      "result: \t 424 \t 1.0 \t 0.104793\n",
      "result: \t 425 \t 1.0 \t 0.14457\n",
      "result: \t 426 \t 1.0 \t 0.332392\n",
      "result: \t 427 \t 1.0 \t 0.0243247\n",
      "result: \t 428 \t 1.0 \t 0.0856002\n",
      "result: \t 429 \t 1.0 \t 0.643144\n",
      "result: \t 430 \t 0.0 \t 0.124393\n",
      "result: \t 431 \t 1.0 \t 0.066817\n",
      "result: \t 432 \t 1.0 \t 0.00680676\n",
      "result: \t 433 \t 1.0 \t 0.627415\n",
      "result: \t 434 \t 1.0 \t 0.236419\n",
      "result: \t 435 \t 0.0 \t 0.475499\n",
      "result: \t 436 \t 0.0 \t 0.166798\n",
      "result: \t 437 \t 0.0 \t 0.395106\n",
      "result: \t 438 \t 1.0 \t 0.492995\n",
      "result: \t 439 \t 1.0 \t 0.00975205\n",
      "result: \t 440 \t 0.0 \t 0.390913\n",
      "result: \t 441 \t 1.0 \t 0.317931\n",
      "result: \t 442 \t 0.0 \t 0.172045\n",
      "result: \t 443 \t 0.0 \t 0.0892659\n",
      "result: \t 444 \t 1.0 \t 0.120068\n",
      "result: \t 445 \t 0.0 \t 0.289819\n",
      "result: \t 446 \t 1.0 \t 0.238855\n",
      "result: \t 447 \t 0.0 \t 0.288481\n",
      "result: \t 448 \t 1.0 \t 0.220307\n",
      "result: \t 449 \t 1.0 \t 0.0416855\n",
      "result: \t 450 \t 0.0 \t 0.0929258\n",
      "result: \t 451 \t 1.0 \t 0.659832\n",
      "result: \t 452 \t 1.0 \t 0.234928\n",
      "result: \t 453 \t 0.0 \t 0.00882445\n",
      "result: \t 454 \t 1.0 \t 0.236419\n",
      "result: \t 455 \t 1.0 \t 0.274205\n",
      "result: \t 456 \t 0.0 \t 0.405695\n",
      "result: \t 457 \t 1.0 \t 0.0856002\n",
      "result: \t 458 \t 0.0 \t 0.15043\n",
      "result: \t 459 \t 0.0 \t 0.470915\n",
      "result: \t 460 \t 0.0 \t 0.103554\n",
      "result: \t 461 \t 1.0 \t 0.462892\n",
      "result: \t 462 \t 0.0 \t 0.127648\n",
      "result: \t 463 \t 0.0 \t 0.284476\n",
      "result: \t 464 \t 1.0 \t 0.15426\n",
      "result: \t 465 \t 1.0 \t 0.384203\n",
      "result: \t 466 \t 1.0 \t 0.384203\n",
      "result: \t 467 \t 1.0 \t 0.0877878\n",
      "result: \t 468 \t 1.0 \t 0.241885\n",
      "result: \t 469 \t 1.0 \t 0.188172\n",
      "result: \t 470 \t 0.0 \t 0.0439283\n",
      "result: \t 471 \t 1.0 \t 0.217083\n",
      "result: \t 472 \t 0.0 \t 0.135986\n",
      "result: \t 473 \t 0.0 \t 0.0767553\n",
      "result: \t 474 \t 1.0 \t 0.31852\n",
      "result: \t 475 \t 1.0 \t 0.316631\n",
      "result: \t 476 \t 1.0 \t 0.0781404\n",
      "result: \t 477 \t 1.0 \t 0.332192\n",
      "result: \t 478 \t 1.0 \t 0.627372\n",
      "result: \t 479 \t 1.0 \t 0.236419\n",
      "result: \t 480 \t 1.0 \t 0.153328\n",
      "result: \t 481 \t 0.0 \t 0.128527\n",
      "result: \t 482 \t 1.0 \t 0.355479\n",
      "result: \t 483 \t 0.0 \t 0.156602\n",
      "result: \t 484 \t 1.0 \t 0.433377\n",
      "result: \t 485 \t 1.0 \t 0.477827\n",
      "result: \t 486 \t 0.0 \t 0.062277\n",
      "result: \t 487 \t 0.0 \t 0.721961\n",
      "result: \t 488 \t 0.0 \t 0.108073\n",
      "result: \t 489 \t 0.0 \t 0.140024\n",
      "result: \t 490 \t 1.0 \t 0.0986527\n",
      "result: \t 491 \t 0.0 \t 0.0859175\n",
      "result: \t 492 \t 0.0 \t 0.36488\n",
      "result: \t 493 \t 1.0 \t 0.264389\n",
      "result: \t 494 \t 0.0 \t 0.010044\n",
      "result: \t 495 \t 1.0 \t 0.541203\n",
      "result: \t 496 \t 0.0 \t 0.405695\n",
      "result: \t 497 \t 0.0 \t 0.686846\n",
      "result: \t 498 \t 1.0 \t 0.451347\n",
      "result: \t 499 \t 1.0 \t 0.331196\n",
      "result: \t 500 \t 1.0 \t 0.355479\n",
      "result: \t 501 \t 1.0 \t 0.691376\n",
      "result: \t 502 \t 1.0 \t 0.147382\n",
      "result: \t 503 \t 1.0 \t 0.154416\n",
      "result: \t 504 \t 1.0 \t 0.372274\n",
      "result: \t 505 \t 1.0 \t 0.316605\n",
      "result: \t 506 \t 0.0 \t 0.115704\n",
      "result: \t 507 \t 1.0 \t 0.0261292\n",
      "result: \t 508 \t 0.0 \t 0.0899417\n",
      "result: \t 509 \t 1.0 \t 0.399939\n",
      "result: \t 510 \t 0.0 \t 0.272728\n",
      "result: \t 511 \t 0.0 \t 0.218013\n",
      "result: \t 512 \t 1.0 \t 0.31852\n",
      "result: \t 513 \t 0.0 \t 0.423798\n",
      "result: \t 514 \t 0.0 \t 0.0708288\n",
      "result: \t 515 \t 1.0 \t 0.154416\n",
      "result: \t 516 \t 1.0 \t 0.126146\n",
      "result: \t 517 \t 0.0 \t 0.272753\n",
      "result: \t 518 \t 1.0 \t 0.331196\n",
      "result: \t 519 \t 1.0 \t 0.154416\n",
      "result: \t 520 \t 0.0 \t 0.0353435\n",
      "result: \t 521 \t 1.0 \t 0.104793\n",
      "result: \t 522 \t 0.0 \t 0.334237\n",
      "result: \t 523 \t 1.0 \t 0.462892\n",
      "result: \t 524 \t 0.0 \t 0.246166\n",
      "result: \t 525 \t 0.0 \t 0.410932\n",
      "result: \t 526 \t 1.0 \t 0.240902\n",
      "result: \t 527 \t 1.0 \t 0.19668\n",
      "result: \t 528 \t 1.0 \t 0.170875\n",
      "result: \t 529 \t 1.0 \t 0.0584177\n",
      "result: \t 530 \t 1.0 \t 0.421897\n",
      "result: \t 531 \t 1.0 \t 0.0986527\n",
      "result: \t 532 \t 1.0 \t 0.983728\n",
      "result: \t 533 \t 1.0 \t 0.308862\n",
      "result: \t 534 \t 0.0 \t 0.151946\n",
      "result: \t 535 \t 1.0 \t 0.331196\n",
      "result: \t 536 \t 0.0 \t 0.062277\n",
      "result: \t 537 \t 0.0 \t 0.128527\n",
      "result: \t 538 \t 0.0 \t 0.248941\n",
      "result: \t 539 \t 1.0 \t 0.147382\n",
      "result: \t 540 \t 0.0 \t 0.0892659\n",
      "result: \t 541 \t 0.0 \t 0.065928\n",
      "result: \t 542 \t 0.0 \t 0.218013\n",
      "result: \t 543 \t 0.0 \t 0.395106\n",
      "result: \t 544 \t 0.0 \t 0.0785337\n",
      "result: \t 545 \t 0.0 \t 0.151946\n",
      "result: \t 546 \t 0.0 \t 0.0494573\n",
      "result: \t 547 \t 0.0 \t 0.394126\n",
      "result: \t 548 \t 0.0 \t 0.0859175\n",
      "result: \t 549 \t 1.0 \t 0.0176077\n",
      "result: \t 550 \t 0.0 \t 0.00882445\n",
      "result: \t 551 \t 1.0 \t 0.452516\n",
      "result: \t 552 \t 1.0 \t 0.14457\n",
      "result: \t 553 \t 1.0 \t 0.22235\n",
      "result: \t 554 \t 1.0 \t 0.516536\n",
      "result: \t 555 \t 1.0 \t 0.0627057\n",
      "result: \t 556 \t 0.0 \t 0.0277512\n",
      "result: \t 557 \t 0.0 \t 0.115704\n",
      "result: \t 558 \t 1.0 \t 0.154416\n",
      "result: \t 559 \t 1.0 \t 0.141949\n",
      "result: \t 560 \t 1.0 \t 0.141949\n",
      "result: \t 561 \t 1.0 \t 0.0386558\n",
      "result: \t 562 \t 0.0 \t 0.0809611\n",
      "result: \t 563 \t 0.0 \t 0.0636775\n",
      "result: \t 564 \t 1.0 \t 0.58433\n",
      "result: \t 565 \t 0.0 \t 0.485448\n",
      "result: \t 566 \t 0.0 \t 0.286326\n",
      "result: \t 567 \t 0.0 \t 0.135018\n",
      "result: \t 568 \t 1.0 \t 0.0855771\n",
      "result: \t 569 \t 0.0 \t 0.0851486\n",
      "result: \t 570 \t 1.0 \t 0.412916\n",
      "result: \t 571 \t 1.0 \t 0.217958\n",
      "result: \t 572 \t 0.0 \t 0.313004\n",
      "result: \t 573 \t 0.0 \t 0.308941\n",
      "result: \t 574 \t 1.0 \t 0.659832\n",
      "result: \t 575 \t 0.0 \t 0.236628\n",
      "result: \t 576 \t 1.0 \t 0.282201\n",
      "result: \t 577 \t 0.0 \t 0.166798\n",
      "result: \t 578 \t 0.0 \t 0.450621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: \t 579 \t 1.0 \t 0.217599\n",
      "result: \t 580 \t 1.0 \t 0.316605\n",
      "result: \t 581 \t 0.0 \t 0.0130374\n",
      "result: \t 582 \t 0.0 \t 0.641334\n",
      "result: \t 583 \t 0.0 \t 0.244196\n",
      "result: \t 584 \t 0.0 \t 0.339582\n",
      "result: \t 585 \t 0.0 \t 0.253308\n",
      "result: \t 586 \t 0.0 \t 0.289844\n",
      "result: \t 587 \t 0.0 \t 0.450621\n",
      "result: \t 588 \t 1.0 \t 0.0194682\n",
      "result: \t 589 \t 0.0 \t 0.259815\n",
      "result: \t 590 \t 1.0 \t 0.244661\n",
      "result: \t 591 \t 0.0 \t 0.145091\n",
      "result: \t 592 \t 0.0 \t 0.0857466\n",
      "result: \t 593 \t 1.0 \t 0.796424\n",
      "result: \t 594 \t 0.0 \t 0.395106\n",
      "result: \t 595 \t 1.0 \t 0.264389\n",
      "result: \t 596 \t 1.0 \t 0.115807\n",
      "result: \t 597 \t 0.0 \t 0.278223\n",
      "result: \t 598 \t 1.0 \t 0.344356\n",
      "result: \t 599 \t 0.0 \t 0.210382\n",
      "result: \t 600 \t 0.0 \t 0.411157\n",
      "result: \t 601 \t 1.0 \t 0.231198\n",
      "result: \t 602 \t 1.0 \t 0.141949\n",
      "result: \t 603 \t 1.0 \t 0.368876\n",
      "result: \t 604 \t 0.0 \t 0.143676\n",
      "result: \t 605 \t 1.0 \t 0.627415\n",
      "result: \t 606 \t 1.0 \t 0.162906\n",
      "result: \t 607 \t 1.0 \t 0.473502\n",
      "result: \t 608 \t 0.0 \t 0.0767553\n",
      "result: \t 609 \t 1.0 \t 0.0386558\n",
      "result: \t 610 \t 0.0 \t 0.339582\n",
      "result: \t 611 \t 1.0 \t 0.546621\n",
      "result: \t 612 \t 1.0 \t 0.566591\n",
      "result: \t 613 \t 0.0 \t 0.00299571\n",
      "result: \t 614 \t 0.0 \t 0.110598\n",
      "result: \t 615 \t 0.0 \t 0.2898\n",
      "result: \t 616 \t 1.0 \t 0.0767959\n",
      "result: \t 617 \t 0.0 \t 0.289717\n",
      "result: \t 618 \t 0.0 \t 0.106187\n",
      "result: \t 619 \t 1.0 \t 0.173787\n",
      "result: \t 620 \t 1.0 \t 0.232513\n",
      "result: \t 621 \t 1.0 \t 0.515248\n",
      "result: \t 622 \t 1.0 \t 0.643144\n",
      "result: \t 623 \t 1.0 \t 0.0721825\n",
      "result: \t 624 \t 0.0 \t 0.478036\n",
      "result: \t 625 \t 0.0 \t 0.011225\n",
      "result: \t 626 \t 1.0 \t 0.281989\n",
      "result: \t 627 \t 0.0 \t 0.135018\n",
      "result: \t 628 \t 0.0 \t 0.470915\n",
      "result: \t 629 \t 0.0 \t 0.127648\n",
      "result: \t 630 \t 0.0 \t 0.450621\n",
      "result: \t 631 \t 0.0 \t 0.21569\n",
      "result: \t 632 \t 1.0 \t 0.063007\n",
      "result: \t 633 \t 0.0 \t 0.735153\n",
      "result: \t 634 \t 1.0 \t 0.610738\n",
      "result: \t 635 \t 0.0 \t 0.259815\n",
      "result: \t 636 \t 1.0 \t 0.0\n",
      "result: \t 637 \t 0.0 \t 0.163116\n",
      "result: \t 638 \t 0.0 \t 0.686846\n",
      "result: \t 639 \t 0.0 \t 0.243627\n",
      "result: \t 640 \t 1.0 \t 0.318688\n",
      "result: \t 641 \t 0.0 \t 0.127648\n",
      "result: \t 642 \t 1.0 \t 0.282201\n",
      "result: \t 643 \t 0.0 \t 0.251173\n",
      "result: \t 644 \t 0.0 \t 0.194357\n",
      "result: \t 645 \t 0.0 \t 0.272728\n",
      "result: \t 646 \t 0.0 \t 0.406361\n",
      "result: \t 647 \t 1.0 \t 0.063007\n",
      "result: \t 648 \t 1.0 \t 0.00384252\n",
      "result: \t 649 \t 1.0 \t 0.063007\n",
      "result: \t 650 \t 0.0 \t 0.485448\n",
      "result: \t 651 \t 1.0 \t 0.566591\n",
      "result: \t 652 \t 1.0 \t 0.1188\n",
      "result: \t 653 \t 1.0 \t 0.154416\n",
      "result: \t 654 \t 1.0 \t 0.308534\n",
      "result: \t 655 \t 0.0 \t 0.107033\n",
      "result: \t 656 \t 0.0 \t 0.010044\n",
      "result: \t 657 \t 1.0 \t 0.388281\n",
      "result: \t 658 \t 1.0 \t 0.0627057\n",
      "result: \t 659 \t 0.0 \t 0.541516\n",
      "result: \t 660 \t 1.0 \t 0.0691199\n",
      "result: \t 661 \t 1.0 \t 0.264389\n",
      "result: \t 662 \t 1.0 \t 0.105786\n",
      "result: \t 663 \t 1.0 \t 0.0606995\n",
      "result: \t 664 \t 1.0 \t 0.120068\n",
      "result: \t 665 \t 0.0 \t 0.314924\n",
      "result: \t 666 \t 0.0 \t 0.541516\n",
      "result: \t 667 \t 0.0 \t 0.0450757\n",
      "result: \t 668 \t 0.0 \t 0.151946\n",
      "result: \t 669 \t 1.0 \t 0.433377\n",
      "result: \t 670 \t 1.0 \t 0.103986\n",
      "result: \t 671 \t 0.0 \t 0.597644\n",
      "result: \t 672 \t 0.0 \t 0.707909\n",
      "result: \t 673 \t 0.0 \t 0.721961\n",
      "result: \t 674 \t 1.0 \t 0.321725\n",
      "result: \t 675 \t 1.0 \t 0.0167903\n",
      "result: \t 676 \t 1.0 \t 0.308873\n",
      "result: \t 677 \t 0.0 \t 0.280865\n",
      "result: \t 678 \t 0.0 \t 0.271647\n",
      "result: \t 679 \t 1.0 \t 0.00975205\n",
      "result: \t 680 \t 0.0 \t 0.545302\n",
      "result: \t 681 \t 0.0 \t 0.259815\n",
      "result: \t 682 \t 1.0 \t 0.162555\n",
      "result: \t 683 \t 0.0 \t 0.237655\n",
      "result: \t 684 \t 0.0 \t 0.106187\n",
      "result: \t 685 \t 1.0 \t 0.201754\n",
      "result: \t 686 \t 1.0 \t 0.217599\n",
      "result: \t 687 \t 0.0 \t 0.406361\n",
      "result: \t 688 \t 1.0 \t 0.0206666\n",
      "result: \t 689 \t 1.0 \t 0.217074\n",
      "result: \t 690 \t 0.0 \t 0.350796\n",
      "result: \t 691 \t 0.0 \t 0.0767553\n",
      "result: \t 692 \t 1.0 \t 0.332691\n",
      "result: \t 693 \t 0.0 \t 0.00828621\n",
      "result: \t 694 \t 0.0 \t 0.735153\n",
      "result: \t 695 \t 0.0 \t 0.106187\n",
      "result: \t 696 \t 0.0 \t 0.411157\n",
      "result: \t 697 \t 1.0 \t 0.338181\n",
      "result: \t 698 \t 1.0 \t 0.24641\n",
      "result: \t 699 \t 1.0 \t 0.0843395\n",
      "result: \t 700 \t 1.0 \t 0.503358\n",
      "result: \t 701 \t 1.0 \t 0.539874\n",
      "result: \t 702 \t 1.0 \t 0.265643\n",
      "result: \t 703 \t 1.0 \t 0.162555\n",
      "result: \t 704 \t 0.0 \t 0.334237\n",
      "result: \t 705 \t 1.0 \t 0.444739\n",
      "result: \t 706 \t 0.0 \t 0.272753\n",
      "result: \t 707 \t 0.0 \t 0.0680388\n",
      "result: \t 708 \t 0.0 \t 0.301519\n",
      "result: \t 709 \t 0.0 \t 0.0785337\n",
      "result: \t 710 \t 1.0 \t 0.627415\n",
      "result: \t 711 \t 0.0 \t 0.597644\n",
      "result: \t 712 \t 1.0 \t 0.627415\n",
      "result: \t 713 \t 0.0 \t 0.00882445\n",
      "result: \t 714 \t 1.0 \t 0.265643\n",
      "result: \t 715 \t 1.0 \t 0.308862\n",
      "result: \t 716 \t 0.0 \t 0.135082\n",
      "result: \t 717 \t 0.0 \t 0.127648\n",
      "result: \t 718 \t 1.0 \t 0.796424\n",
      "result: \t 719 \t 1.0 \t 0.494427\n"
     ]
    }
   ],
   "source": [
    "print(yy_test[0])\n",
    "pred_1 = testmod_1.predict(xx_train[None, 0,0,:,:,None])\n",
    "\n",
    "tt = 100\n",
    "labs1 = []\n",
    "labs0 = []\n",
    "print(yy_test[tt])\n",
    "pred_2 = testmod_1.predict(xx_train[None,tt,0,:,:,None])\n",
    "\n",
    "for i in range(len(xx_train)):\n",
    "    pred_2 = testmod_1.predict(xx_train[None,i,0,:,:,None])\n",
    "    if np.linalg.norm(yy_train[i] - 1) == 0:\n",
    "        labs1.append(i)\n",
    "    else:\n",
    "        labs0.append(i)\n",
    "    print('result:','\\t', i,'\\t', yy_train[i],'\\t', np.linalg.norm(pred_1-pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_1: 103.66836398\n",
      "acc_0: 95.829120707\n"
     ]
    }
   ],
   "source": [
    "acc_1 = 0\n",
    "for i in range(len(labs1)):\n",
    "    acc_1 += np.linalg.norm(pred_1-testmod_1.predict(xx_train[None,labs1[i],0,:,:,None]))\n",
    "\n",
    "acc_0 = 0\n",
    "for i in range(len(labs0)):\n",
    "    acc_0 += np.linalg.norm(pred_1-testmod_1.predict(xx_train[None,labs0[i],0,:,:,None]))\n",
    "\n",
    "print('acc_1:', acc_1)\n",
    "print('acc_0:', acc_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n",
      "[12.912468, 11.579817, 11.073095, 11.582479, 12.009239, 11.521235, 11.8072, 11.009918, 10.818402, 11.326399, 12.399729, 12.086484, 10.278913, 11.051972]\n"
     ]
    }
   ],
   "source": [
    "pred_2 = []\n",
    "print(y_test[0])\n",
    "pred_1 = testmod_1.predict(x_train[None,0,:,:,None])\n",
    "\n",
    "for i in range(1,15):\n",
    "    pred_2.append(np.linalg.norm(testmod_1.predict(x_train[None,i,:,:,None])))\n",
    "\n",
    "# print('result:', np.linalg.norm(pred_1-pred_2))\n",
    "print([np.linalg.norm(pred_1-pred_2[i]) for i in range(len(pred_2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(539, 60, 128, 1)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train[:,0,:,:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 128)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.01094961,  1.01399767,  1.01906908,  1.01613176], dtype=float32),\n",
       " array([  1.85610606e-05,   2.14835541e-06,   5.89442266e-07,\n",
       "         -1.97443410e-06], dtype=float32),\n",
       " array([  5.95167112e-05,   2.45559531e-05,   5.38175373e-05,\n",
       "          6.08851260e-05], dtype=float32),\n",
       " array([ 0.14509232,  0.11979525,  0.13534319,  0.13674594], dtype=float32)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.layers[3].layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def testtest():\n",
    "#     #build convnet to use in each siamese 'leg'\n",
    "#     input_shape = (64, 128, 1)\n",
    "#     convnet = Sequential()\n",
    "#     convnet.add(Conv2D(64,(10,10),activation='relu', input_shape=input_shape))\n",
    "#     convnet.add(Conv2D(128,(7,7),activation='relu'))\n",
    "#     convnet.add(Flatten())\n",
    "#     convnet.add(Dense(2,activation=\"sigmoid\"))\n",
    "#     return convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #build convnet to use in each siamese 'leg'\n",
    "# input_shape = (64, 128, 1)\n",
    "# convnet = Sequential()\n",
    "# convnet.add(Conv2D(64,(10,10),activation='relu', input_shape=input_shape))\n",
    "# convnet.add(Conv2D(128,(7,7),activation='relu'))\n",
    "# convnet.add(Flatten())\n",
    "# convnet.add(Dense(2,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 1, 60, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 4, 60, 128)        500       \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 4, 60, 128)        16        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_10 (Depthwi (None, 8, 1, 128)         480       \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 8, 1, 128)         32        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 8, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_14 (Averag (None, 8, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_7 (Separabl (None, 8, 1, 32)          192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 1, 32)          32        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 8, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_15 (Averag (None, 8, 1, 4)           0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 8, 1, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,318\n",
      "Trainable params: 1,278\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def EEGNet(nb_classes, Chans = 64, Samples = 128, \n",
    "             dropoutRate = 0.25, kernLength = 64, F1 = 4, \n",
    "             D = 2, F2 = 8, dropoutType = 'Dropout'):\n",
    "    \"\"\" Keras Implementation of EEGNet (https://arxiv.org/abs/1611.08024)\n",
    "\n",
    "    Inputs:\n",
    "        \n",
    "      nb_classes      : int, number of classes to classify\n",
    "      Chans, Samples  : number of channels and time points in the EEG data\n",
    "      dropoutRate     : dropout fraction\n",
    "      kernLength      : length of temporal convolution in first layer. We found\n",
    "                        that setting this to be half the sampling rate worked\n",
    "                        well in practice. For the SMR dataset in particular\n",
    "                        since the data was high-passed at 4Hz we used a kernel\n",
    "                        length of 32.     \n",
    "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
    "                        filters (F2) to learn. Default: F1 = 4, F2 = F1 * D. \n",
    "      D               : number of spatial filters to learn within each temporal\n",
    "                        convolution. Default: D = 2\n",
    "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "    \n",
    "    input1   = Input(shape = (1, Chans, Samples))\n",
    "\n",
    "    ##################################################################\n",
    "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
    "                                   input_shape = (1, Chans, Samples),\n",
    "                                   use_bias = False, data_format='channels_first')(input1)\n",
    "    block1       = BatchNormalization(axis = 1)(block1)\n",
    "    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
    "                                   depth_multiplier = D,\n",
    "                                   depthwise_constraint = max_norm(1.), data_format='channels_first')(block1)\n",
    "    block1       = BatchNormalization(axis = 1)(block1)\n",
    "    block1       = Activation('elu')(block1)\n",
    "    block1       = AveragePooling2D((1, 4), data_format='channels_first')(block1)\n",
    "    block1       = dropoutType(dropoutRate)(block1)\n",
    "    \n",
    "    block2       = SeparableConv2D(F2, (1, 16),\n",
    "                                   use_bias = False,\n",
    "                                   padding = 'same',\n",
    "                                   data_format='channels_first')(block1)\n",
    "    block2       = BatchNormalization(axis = 1)(block2)\n",
    "    block2       = Activation('elu')(block2)\n",
    "    block2       = AveragePooling2D((1, 8), data_format='channels_first')(block2)\n",
    "    block2       = dropoutType(dropoutRate)(block2)\n",
    "        \n",
    "    flatten      = Flatten(name = 'flatten')(block2)\n",
    "    \n",
    "    dense        = Dense(nb_classes, name = 'dense', \n",
    "                         kernel_constraint = max_norm(0.25))(flatten)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input1, outputs=softmax)\n",
    "\n",
    "model  = EEGNet(nb_classes = 2,\n",
    "                Chans = 60,\n",
    "                Samples = 128,\n",
    "                kernLength = 125)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
